{
	"name": "Daily_Running_PublicBlob_Pipeline",
	"properties": {
		"description": "This pipeline contains Posts data. Which come from Public Blob Storage. \nIn addition, it will be updated every day in specific time that will schedule it.\n\n",
		"activities": [
			{
				"name": "Copy posts data",
				"description": "Copy the posts data from the source file, to the destination folder in the datalake. ",
				"type": "Copy",
				"dependsOn": [
					{
						"activity": "Delete posts data",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "ParquetSource",
						"storeSettings": {
							"type": "AzureBlobStorageReadSettings",
							"recursive": true,
							"wildcardFileName": "*.parquet",
							"enablePartitionDiscovery": false
						},
						"formatSettings": {
							"type": "ParquetReadSettings"
						}
					},
					"sink": {
						"type": "ParquetSink",
						"storeSettings": {
							"type": "AzureBlobStorageWriteSettings"
						},
						"formatSettings": {
							"type": "ParquetWriteSettings"
						}
					},
					"enableStaging": false,
					"translator": {
						"type": "TabularTranslator",
						"typeConversion": true,
						"typeConversionSettings": {
							"allowDataTruncation": true,
							"treatBooleanAsNumber": false
						}
					}
				},
				"inputs": [
					{
						"referenceName": "Source_Posts_PublicBlob_WCD",
						"type": "DatasetReference"
					}
				],
				"outputs": [
					{
						"referenceName": "DestinationDS_Posts_Datalake",
						"type": "DatasetReference"
					}
				]
			},
			{
				"name": "Delete posts data",
				"description": "delete posts data that exists in the posts folder in the datalake(destination) that is referred to prevoius day.",
				"type": "Delete",
				"dependsOn": [
					{
						"activity": "Copy Archive Posts",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"dataset": {
						"referenceName": "DestinationDS_Posts_Datalake",
						"type": "DatasetReference"
					},
					"logStorageSettings": {
						"linkedServiceName": {
							"referenceName": "Linkeserviced_Datalake",
							"type": "LinkedServiceReference"
						},
						"path": "capstoneproject2-group8-container/Landing_DataLake_Folders/logs/Posts_Logs"
					},
					"enableLogging": true,
					"storeSettings": {
						"type": "AzureBlobStorageReadSettings",
						"recursive": true,
						"enablePartitionDiscovery": false
					}
				}
			},
			{
				"name": "Copy Archive Posts",
				"description": "It is copy data from the posts folder in datalake to store it into achive posts folder. That is help to have historical data.  ",
				"type": "Copy",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "ParquetSource",
						"storeSettings": {
							"type": "AzureBlobStorageReadSettings",
							"recursive": true,
							"wildcardFileName": "*.parquet",
							"enablePartitionDiscovery": false
						},
						"formatSettings": {
							"type": "ParquetReadSettings"
						}
					},
					"sink": {
						"type": "ParquetSink",
						"storeSettings": {
							"type": "AzureBlobStorageWriteSettings"
						},
						"formatSettings": {
							"type": "ParquetWriteSettings"
						}
					},
					"enableStaging": false,
					"translator": {
						"type": "TabularTranslator",
						"typeConversion": true,
						"typeConversionSettings": {
							"allowDataTruncation": true,
							"treatBooleanAsNumber": false
						}
					}
				},
				"inputs": [
					{
						"referenceName": "DestinationDS_Posts_Datalake",
						"type": "DatasetReference"
					}
				],
				"outputs": [
					{
						"referenceName": "Archive_Daily_Posts",
						"type": "DatasetReference"
					}
				]
			},
			{
				"name": "Run NLP",
				"description": "In This \"ML_Sentiment_Analysis_NLP\" notebook we'll run the ML Model on data of \"Posts_destination_datalake\" ",
				"type": "DatabricksNotebook",
				"dependsOn": [
					{
						"activity": "Copy posts data",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebookPath": "/Users/1707458@stu.kau.edu.sa/ML_Sentiment_Analysis_NLP"
				},
				"linkedServiceName": {
					"referenceName": "LinkedService_Databricks_NLP_posts",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "Convert Parquet to CSV",
				"type": "DatabricksNotebook",
				"dependsOn": [
					{
						"activity": "Copy posts data",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebookPath": "/Users/1707458@stu.kau.edu.sa/Convert_Posts_Parquet_2_CSV"
				},
				"linkedServiceName": {
					"referenceName": "LinkedService_Databricks_NLP_posts",
					"type": "LinkedServiceReference"
				}
			}
		],
		"annotations": [],
		"lastPublishTime": "2024-05-09T15:27:24Z"
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}
{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "CaptoneProject2-Group8-ADF"
		},
		"LinkedService_Archive_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LinkedService_Archive'"
		},
		"LinkedService_ConvertTypes_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LinkedService_ConvertTypes'"
		},
		"LinkedService_Databricks_NLP_posts_accessToken": {
			"type": "secureString",
			"metadata": "Secure string for 'accessToken' of 'LinkedService_Databricks_NLP_posts'"
		},
		"LinkedService_PublicBlob_WCD_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LinkedService_PublicBlob_WCD'"
		},
		"LinkedService_RDS_Postgres_password": {
			"type": "secureString",
			"metadata": "Secure string for 'password' of 'LinkedService_RDS_Postgres'"
		},
		"Linkeserviced_Datalake_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'Linkeserviced_Datalake'"
		},
		"LinkedService_Databricks_NLP_posts_properties_typeProperties_existingClusterId": {
			"type": "string",
			"defaultValue": "0510-165754-cf53jv82"
		},
		"LinkedService_RDS_Postgres_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "de-rds.czm23kqmbd6o.ca-central-1.rds.amazonaws.com"
		},
		"LinkedService_RDS_Postgres_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "stack"
		},
		"LinkedService_RDS_Postgres_properties_typeProperties_username": {
			"type": "string",
			"defaultValue": "postgres"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/LinkedService_Archive')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('LinkedService_Archive_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/LinkedService_ConvertTypes')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "use t to convert the types of files in datalake ==> csv file ",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('LinkedService_ConvertTypes_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/LinkedService_Databricks_NLP_posts')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Link the pipeline with the Databricks notebook for NLP",
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://adb-560590091399003.3.azuredatabricks.net",
					"accessToken": {
						"type": "SecureString",
						"value": "[parameters('LinkedService_Databricks_NLP_posts_accessToken')]"
					},
					"existingClusterId": "[parameters('LinkedService_Databricks_NLP_posts_properties_typeProperties_existingClusterId')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/LinkedService_PublicBlob_WCD')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This linked service is to connect between, the blob storage of WeCloudData <--> Daily Pipeline for Posts.",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('LinkedService_PublicBlob_WCD_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/LinkedService_RDS_Postgres')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This linked service to connect between data in RDS Postgres <--with--> the weekly pipeline.",
				"annotations": [],
				"type": "PostgreSqlV2",
				"typeProperties": {
					"server": "[parameters('LinkedService_RDS_Postgres_properties_typeProperties_server')]",
					"port": 5432,
					"database": "[parameters('LinkedService_RDS_Postgres_properties_typeProperties_database')]",
					"username": "[parameters('LinkedService_RDS_Postgres_properties_typeProperties_username')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('LinkedService_RDS_Postgres_password')]"
					},
					"sslMode": 2,
					"authenticationType": "Basic"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Linkeserviced_Datalake')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('Linkeserviced_Datalake_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Archive_Daily_Posts')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Linkeserviced_Datalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "Archive/Archive_Posts",
						"container": "capstoneproject2-group8-container"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Archive_Weekly_Posttype')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Linkeserviced_Datalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "Archive/Archive_PostType",
						"container": "capstoneproject2-group8-container"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Archive_Weekly_Users')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Linkeserviced_Datalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "Archive/Archive_Users",
						"container": "capstoneproject2-group8-container"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/DestinationDS_PostType_Datalake')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "After copying the data from source \"posttype\" we will store it in the Datalake in specific folder for \"posttype\" which is the destination.",
				"linkedServiceName": {
					"referenceName": "Linkeserviced_Datalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "Landing_DataLake_Folders/PostTypes_Destination_Datalake",
						"container": "capstoneproject2-group8-container"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/DestinationDS_Posts_Datalake')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "After copying the data from source \"Source_posts\" we will store it in the Datalake in specific folder for \"Destination_posts\" which is the destination.",
				"linkedServiceName": {
					"referenceName": "Linkeserviced_Datalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "Landing_DataLake_Folders/Posts_Destination_Datalake",
						"container": "capstoneproject2-group8-container"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/DestinationDS_Users_Datalake')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "After copying th data from source we will store it in the Datalake which is the destination.",
				"linkedServiceName": {
					"referenceName": "Linkeserviced_Datalake",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "Landing_DataLake_Folders/Users_Destination_Datalake",
						"container": "capstoneproject2-group8-container"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Source_PostType_RDS_postgres')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This is the data source for StackOverFlow PostType it will upload here . It comes from RDS postgres.",
				"linkedServiceName": {
					"referenceName": "LinkedService_RDS_Postgres",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "PostgreSqlV2Table",
				"schema": [],
				"typeProperties": {
					"schema": "raw_st",
					"table": "posttypes"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/LinkedService_RDS_Postgres')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Source_Posts_PublicBlob_WCD')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "The dataset from the source WCD PublicBlob, will stored her ",
				"linkedServiceName": {
					"referenceName": "LinkedService_PublicBlob_WCD",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "*.parquet",
						"folderPath": "archive",
						"container": "de-project-st"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/LinkedService_PublicBlob_WCD')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Source_Users_RDS_postgres')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This is the data source for StackOverFlow Users will upload here . It comes from RDS postgres.",
				"linkedServiceName": {
					"referenceName": "LinkedService_RDS_Postgres",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "PostgreSqlV2Table",
				"schema": [],
				"typeProperties": {
					"schema": "raw_st",
					"table": "users"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/linkedServices/LinkedService_RDS_Postgres')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Daily_Running_PublicBlob_Pipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This pipeline contains Posts data. Which come from Public Blob Storage. \nIn addition, it will be updated every day in specific time that will schedule it.\n\n",
				"activities": [
					{
						"name": "Copy posts data to Datalake",
						"description": "Copy the posts data from the source file, to the destination folder in the datalake. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete posts data from Datalake",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*.parquet",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "ParquetReadSettings"
								}
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "Source_Posts_PublicBlob_WCD",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDS_Posts_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delete posts data from Datalake",
						"description": "delete posts data that exists in the posts folder in the datalake(destination) that is referred to prevoius day.",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Copy Archive Posts",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DestinationDS_Posts_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							},
							"logStorageSettings": {
								"linkedServiceName": {
									"referenceName": "Linkeserviced_Datalake",
									"type": "LinkedServiceReference"
								},
								"path": "capstoneproject2-group8-container/Landing_DataLake_Folders/logs/Posts_Logs"
							},
							"enableLogging": true,
							"storeSettings": {
								"type": "AzureBlobStorageReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Copy Archive Posts",
						"description": "It is copy data from the posts folder in datalake to store it into achive posts folder. That is help to have historical data.  ",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*.parquet",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "ParquetReadSettings"
								}
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "DestinationDS_Posts_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Archive_Daily_Posts",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Run NLP",
						"description": "In This \"ML_Sentiment_Analysis_NLP\" notebook we'll run the ML Model on data of \"Posts_destination_datalake\" ",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "Copy posts data to Datalake",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Users/1707458@stu.kau.edu.sa/CP2_G8-ML_Sentiment_Analysis_NLP"
						},
						"linkedServiceName": {
							"referenceName": "LinkedService_Databricks_NLP_posts",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Convert Posts from Parquet to CSV",
						"description": "Here we will Convert Posts from Parquet to CSV to use it in the Visualization ",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "Copy posts data to Datalake",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Users/1707458@stu.kau.edu.sa/CP2_G8-Convert_Posts_Parquet_2_CSV"
						},
						"linkedServiceName": {
							"referenceName": "LinkedService_Databricks_NLP_posts",
							"type": "LinkedServiceReference"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-05-09T15:27:24Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/Source_Posts_PublicBlob_WCD')]",
				"[concat(variables('factoryId'), '/datasets/DestinationDS_Posts_Datalake')]",
				"[concat(variables('factoryId'), '/datasets/Archive_Daily_Posts')]",
				"[concat(variables('factoryId'), '/linkedServices/LinkedService_Databricks_NLP_posts')]",
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Weekly_Running_RDS_Pipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This pipeline contains both Users and PostType data. Which come from Amazon RDS. \nIn addition, it will be updated every week in specific time that will schedule it.",
				"activities": [
					{
						"name": "Copy Users Data to Datalake",
						"description": "Copy the users data from the source file, to the destination folder in the datalake. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete Users Datalake",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "PostgreSqlV2Source"
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "Source_Users_RDS_postgres",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDS_Users_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy PostType Data to Datalake",
						"description": "Copy the posttype data from the source file, to the destination folder in the datalake. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete PostType Datalake",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "PostgreSqlV2Source"
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "Source_PostType_RDS_postgres",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDS_PostType_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Archive PostType",
						"description": "It copies data from the destination posttype folder in datalake to store it into archive posttype folder. That is helpful to have historical data.  ",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "DestinationDS_PostType_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Archive_Weekly_Posttype",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Archive Users",
						"description": "It copies data from the destination users folder in datalake to store it into archive users folder. That is helpful to have historical data.  ",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "DestinationDS_Users_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Archive_Weekly_Users",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delete Users Datalake",
						"description": "delete users' data that exists in the destination folder of users in the datalake (destination) that is referred to the previous week.",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Copy Archive Users",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DestinationDS_Users_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							},
							"logStorageSettings": {
								"linkedServiceName": {
									"referenceName": "Linkeserviced_Datalake",
									"type": "LinkedServiceReference"
								},
								"path": "capstoneproject2-group8-container/Landing_DataLake_Folders/logs/Users_Logs"
							},
							"enableLogging": true,
							"storeSettings": {
								"type": "AzureBlobStorageReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete PostType Datalake",
						"description": "delete posttype data that exists in the destination folder of posttype in the datalake (destination) that is referred to previous week.",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Copy Archive PostType",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DestinationDS_PostType_Datalake",
								"type": "DatasetReference",
								"parameters": {}
							},
							"logStorageSettings": {
								"linkedServiceName": {
									"referenceName": "Linkeserviced_Datalake",
									"type": "LinkedServiceReference"
								},
								"path": "capstoneproject2-group8-container/Landing_DataLake_Folders/logs/PostType_Logs"
							},
							"enableLogging": true,
							"storeSettings": {
								"type": "AzureBlobStorageReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Convert Usres from Text to CSV",
						"description": "Here we will Convert Users from Text to CSV to use it in the Visualization ",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "Copy Users Data to Datalake",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Users/1707458@stu.kau.edu.sa/Convert_Usres_text_2_CSV"
						},
						"linkedServiceName": {
							"referenceName": "LinkedService_Databricks_NLP_posts",
							"type": "LinkedServiceReference"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-05-09T15:27:24Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/Source_Users_RDS_postgres')]",
				"[concat(variables('factoryId'), '/datasets/DestinationDS_Users_Datalake')]",
				"[concat(variables('factoryId'), '/datasets/Source_PostType_RDS_postgres')]",
				"[concat(variables('factoryId'), '/datasets/DestinationDS_PostType_Datalake')]",
				"[concat(variables('factoryId'), '/datasets/Archive_Weekly_Posttype')]",
				"[concat(variables('factoryId'), '/datasets/Archive_Weekly_Users')]",
				"[concat(variables('factoryId'), '/linkedServices/LinkedService_Databricks_NLP_posts')]",
				"[concat(variables('factoryId'), '/linkedServices/Linkeserviced_Datalake')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Daily_Running_Posts')]",
			"type": "Microsoft.DataFactory/factories/triggers",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "The posts will run everyday, in a specific time that we will provide",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Daily_Running_PublicBlob_Pipeline",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2024-05-11T18:12:00",
						"endTime": "2024-05-18T15:00:00",
						"timeZone": "Arab Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								9
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/pipelines/Daily_Running_PublicBlob_Pipeline')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Weekly_Running_Users_PostType')]",
			"type": "Microsoft.DataFactory/factories/triggers",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "The Users and PostType will run every week, in a specific time that we will provide",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Weekly_Running_RDS_Pipeline",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2024-05-11T18:23:00",
						"timeZone": "E. Africa Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								9
							],
							"weekDays": [
								"Sunday"
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/pipelines/Weekly_Running_RDS_Pipeline')]"
			]
		}
	]
}